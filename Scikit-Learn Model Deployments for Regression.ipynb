{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Model Deployments for SVR, RF, Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "from Crypto.Cipher import AES\n",
    "from sklearn.tree import _tree\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Decision-Tree as List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_list(tree, feature_names, num):\n",
    "    code_list = ''\n",
    "    \n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    #code_list = '[input feature, node, [input feature, node, left leaf, right leaf], right leaf]'\n",
    "    def recurse(node, depth, code_list):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            code_list = code_list + \"[{},{},\".format(name, threshold)\n",
    "            code_list = recurse(tree_.children_left[node], depth + 1, code_list)\n",
    "            code_list = code_list + \",\"\n",
    "            code_list = recurse(tree_.children_right[node], depth + 1, code_list)\n",
    "            code_list = code_list + \"]\"\n",
    "        else:\n",
    "            code_list = code_list + \"{}\".format(tree_.value[node][0][0])\n",
    "        \n",
    "        return code_list\n",
    "    code_list = recurse(0, 2, code_list)\n",
    "    \n",
    "    return code_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Decision-Tree in List format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict_list(x_input_l, lst):\n",
    "    \"\"\"\n",
    "    Function to get the prediction value where trees are stored in list format recursively\n",
    "    Format: [input feature (column index), node value, left leaf (yes), right leaf (no)]\n",
    "    Input: x_input_l (input for obaining the tree prediction), lst (tree in list format)\n",
    "    Output: pred (final pred value for the provided input)\n",
    "    \"\"\"\n",
    "    if isinstance(lst, (float, int)):\n",
    "        return lst\n",
    "\n",
    "    if isinstance(lst[2], list) and len(lst[2]) == 4:\n",
    "        left_leaf = tree_predict_list(x_input_l, lst[2])\n",
    "    else:\n",
    "        left_leaf = lst[2]\n",
    "    if isinstance(lst[3], list) and len(lst[3]) == 4:\n",
    "        right_leaf = tree_predict_list(x_input_l, lst[3])\n",
    "    else:\n",
    "        right_leaf = lst[3]\n",
    "\n",
    "    # If input feature <= node value then left leaf else right leaf\n",
    "    pred = left_leaf if x_input_l[lst[0]] <= lst[1] else right_leaf\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_prediction(predictions, weights):\n",
    "    # Obtain the index of predictions in increasing order\n",
    "    sorted_idx = np.argsort(predictions) \n",
    "    # Calculate cumulative sum of weights sorted in the above order\n",
    "    weight_cdf =  np.array([x for x in np.cumsum([weights[i]  for  i in sorted_idx])])\n",
    "     # Obtain the median position\n",
    "    median_or_above = weight_cdf >= 0.5 * weight_cdf[-1]\n",
    "    median_idx = median_or_above.argmax()\n",
    "    # Index of prediction for the median position\n",
    "    median_estimators = sorted_idx[median_idx]\n",
    "    return predictions[median_estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypt & Decrypt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_json(json_obj):\n",
    "    \"\"\"\n",
    "    Scikit-Learn Model in JSON format is converted into encrypted JSON.\n",
    "    Input: JSON object\n",
    "    Output: Encrypted JSON.\n",
    "    \"\"\"\n",
    "    secret_key = b'abcdefghijklmnop'    \n",
    "    iv = b'abcdefghijklmKEY'\n",
    "    \n",
    "    BS = 16\n",
    "    pad = lambda s: s + (BS - len(s) % BS) * chr(BS - len(s) % BS)    \n",
    "    cipher = AES.new(secret_key,AES.MODE_CBC,iv)\n",
    "    json_string = json.dumps(json_obj)\n",
    "    json_string = pad(json_string)\n",
    "    ENCODED= (cipher.encrypt(json_string.encode(\"utf8\")))\n",
    "    return ENCODED\n",
    "    \n",
    "def decrypt_json(model_name):\n",
    "    \"\"\"\n",
    "    Models saved as encrypted json files are loaded and decrypted\n",
    "    Input: model_name (name of model to be loaded from saved_models folder)\n",
    "    Output: decrpyted model in json/dictionary format\n",
    "    \"\"\"\n",
    "    folder_path = \"saved_models\\\\\"\n",
    "    filename = folder_path + model_name\n",
    "\n",
    "    unpad = lambda s: s[0:-s[-1]]\n",
    "\n",
    "    secret_key = b'abcdefghijklmnop'\n",
    "    init_vector = b'abcdefghijklmKEY'\n",
    "    with open(filename + '_json_enc.txt', 'rb') as op_file:\n",
    "        json_enc = op_file.read()\n",
    "    cipher = AES.new(secret_key, AES.MODE_CBC, init_vector)\n",
    "    decoded = cipher.decrypt(json_enc)\n",
    "    prog_block = unpad(decoded)\n",
    "    return json.loads(prog_block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save function: SVR/RF/Boosting model --> JSON --> Encrypted JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_json(folder_path, model_name, model_save, model_input, model_output, model_type):\n",
    "    model_par_len = model_input.shape[1]\n",
    "    model_X_mean = list(model_input.mean(axis = 0))\n",
    "    model_X_std = list(model_input.std(axis = 0))\n",
    "    model_X_var = ((model_input-model_input.mean(axis = 0))/model_input.std(axis = 0)).var()\n",
    "    \n",
    "    py_filename_json = folder_path + model_name + '_class.json'\n",
    "    py_filename_enc = folder_path + model_name + '_json_enc.txt'\n",
    "    \n",
    "    dict_file = {}\n",
    "    if model_type == 'SVR':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['kernel'] = model_save[1].kernel\n",
    "        dict_file['degree'] = model_save[1].degree\n",
    "        dict_file['gamma'] = model_save[1].gamma\n",
    "        dict_file['coef0'] = model_save[1].coef0\n",
    "        dict_file['epsilon'] = model_save[1].epsilon\n",
    "        dict_file['n_features'] = model_par_len\n",
    "        dict_file['input_variance'] = model_X_var\n",
    "        dict_file['intercept'] = model_save[1].intercept_[0]\n",
    "        dict_file['mean_value'] = model_X_mean\n",
    "        dict_file['std_value'] = model_X_std\n",
    "        dict_file['dual_coef'] = model_save[1].dual_coef_.tolist()\n",
    "        dict_file['support_vectors'] = model_save[1].support_vectors_.tolist()\n",
    "    elif model_type == 'RF':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['mean_value'] = model_X_mean\n",
    "        dict_file['std_value'] = model_X_std\n",
    "        for i in range(model_save[1].n_estimators):\n",
    "            tree_list = tree_to_list(model_save[1].estimators_[i], [j for j in range(model_par_len)], i) \n",
    "            dict_file['tree_'+ str(i)] = json.loads(tree_list)\n",
    "    elif model_type == 'GradientBoost':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['learning_rate'] = model_save[1].learning_rate\n",
    "        dict_file['base_estimator_value'] = model_output.mean()\n",
    "        dict_file['mean_value'] = model_X_mean\n",
    "        dict_file['std_value'] = model_X_std\n",
    "        for i in range(model_save[1].n_estimators):\n",
    "            tree_list = tree_to_list(model_save[1].estimators_[i][0], [j for j in range(model_par_len)], i) \n",
    "            dict_file['tree_'+ str(i)] = json.loads(tree_list)\n",
    "    elif model_type == 'AdaBoost':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['estimator_weights'] = model_save[1].estimator_weights_.tolist()\n",
    "        dict_file['mean_value'] = model_X_mean\n",
    "        dict_file['std_value'] = model_X_std\n",
    "        for i in range(model_save[1].n_estimators):\n",
    "            tree_list = tree_to_list(model_save[1].estimators_[i], [j for j in range(model_par_len)], i) \n",
    "            dict_file['tree_'+ str(i)] = json.loads(tree_list)\n",
    "    \n",
    "    with open(py_filename_json, \"w\") as write_file:\n",
    "        json.dump(dict_file, write_file)\n",
    "    \n",
    "    json_ciphertext = encrypt_json(dict_file)\n",
    "    \n",
    "    with open(py_filename_enc, \"wb\") as write_file:\n",
    "        write_file.write(json_ciphertext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function: SVR/RF/Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict_json(model, x_input):\n",
    "    '''\n",
    "    Machine Learning models predict function is recreated using the stored values\n",
    "    '''\n",
    "    output_list = []\n",
    "    # SVR Model\n",
    "    if model['model_type'] == 'SVR':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - model['mean_value'])/model['std_value']\n",
    "                # Model support vectors, co-efficients & intercept\n",
    "                sup_vecs = np.array(model['support_vectors'])\n",
    "                dual_coefs = np.array(model['dual_coef'])\n",
    "                intercept = model['intercept']\n",
    "                n_featrs = model['n_features']\n",
    "                inp_var = model['input_variance']\n",
    "                gam_in = model['gamma']\n",
    "                gamma = (1/n_featrs) if gam_in == 'auto' else \\\n",
    "                ((1 / (n_featrs * inp_var)) if gam_in == 'scale' else float(gam_in))\n",
    "                if model['kernel'] == 'linear':\n",
    "                    kernal_out = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                    sample_pred = (np.dot(dual_coefs, kernal_out) + intercept).flatten()[0]\n",
    "                elif model['kernel'] == 'rbf':\n",
    "                    diff = sup_vecs - x_input_sample\n",
    "                    sup_vecs_len = np.shape(sup_vecs)[0]\n",
    "                    norm_val = np.array([np.linalg.norm(diff[n, :]) for n in range(sup_vecs_len)])\n",
    "                    kernal_out = np.exp(-gamma*(norm_val**2))\n",
    "                    sample_pred = (np.dot(dual_coefs, kernal_out) + intercept).flatten()[0]\n",
    "                elif model['kernel'] == 'sigmoid':\n",
    "                    input_support = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                    kernal_out = np.tanh(gamma * input_support + model['coef0'])\n",
    "                    sample_pred = (np.dot(dual_coefs, kernal_out)+ intercept).flatten()[0]\n",
    "                elif model['kernel'] == 'poly':\n",
    "                    input_support = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                    kernal_out = (gamma * input_support + model['coef0'])**model['degree']\n",
    "                    sample_pred = (np.dot(dual_coefs, kernal_out)+ intercept).flatten()[0]\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # RF Model\n",
    "    elif model['model_type'] == 'RF':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - model['mean_value'])/model['std_value']\n",
    "                # Final prediction would be mean of all tree's prediction\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                sample_pred = np.mean([tree_predict_list(x_input_sample, model[t]) for t in trees])\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # Gradient Boosting Model\n",
    "    elif model['model_type'] == 'GradientBoost':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - model['mean_value'])/model['std_value']\n",
    "                base_value = model['base_estimator_value']\n",
    "                learning_rate = model['learning_rate']\n",
    "                # Final prediction would be sum of base estimator value and\n",
    "                # (learning rate * tree's prediction)\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                sample_pred_x = [tree_predict_list(x_input_sample, model[t]) for t in trees]\n",
    "                sample_pred_x = np.array(sample_pred_x).flatten()\n",
    "                sample_pred = sum([base_value] + [learning_rate * x for x in sample_pred_x])\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # Adaptive Boosting Model\n",
    "    elif model['model_type'] == 'AdaBoost':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - model['mean_value'])/model['std_value']\n",
    "                #base_value = model['base_estimator_value']\n",
    "                weights = np.array(model['estimator_weights'])\n",
    "                # Final prediction would be sum of base estimator value and\n",
    "                # (learning rate * tree's prediction)\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                sample_pred_x = [tree_predict_list(x_input_sample, model[t]) for t in trees]\n",
    "                sample_pred_x = np.array(sample_pred_x).flatten()\n",
    "                sample_pred = adaboost_prediction(sample_pred_x, weights)\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "            \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [ 0.5848745   0.99575319  0.10837841  6.01912379  1.76783019 -0.87700594\n",
      "  0.18444025  1.44312873  3.02524518  0.31005461]\n",
      "Custom prediction:  [0.5848744971406905, 0.9957531922203875, 0.10837841372728874, 6.019123787476273, 1.7678301909085776, -0.8770059418189238, 0.184440246546119, 1.4431287258695673, 3.0252451847636, 0.3100546120939154]\n"
     ]
    }
   ],
   "source": [
    "# SVR testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = SVR(kernel='sigmoid', degree=3, \n",
    "           gamma='scale', coef0=0.0, \n",
    "           tol=0.001, C=5.0, epsilon=0.1)\n",
    "model_SVR_pl = Pipeline([('standardize',StandardScaler()),('svr',algo)])\n",
    "model_SVR_sk = model_SVR_pl.fit(X, y)\n",
    "print('Sklearn prediction: ', model_SVR_sk.predict(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_svr', model_SVR_sk, X, y,'SVR')\n",
    "test_model_svr = decrypt_json('test_model_svr')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_svr, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [ 1.29427804  0.53915177  1.08106409  1.83584181  1.17171125 -0.27853926\n",
      "  0.93237289  0.29069576  0.50160982  0.47720693]\n",
      "Custom prediction:  [1.294278038257052, 0.5391517739804317, 1.081064088192825, 1.8358418101984473, 1.1717112498802684, -0.27853925943607927, 0.9323728858918798, 0.2906957610455923, 0.5016098185450591, 0.4772069308768816]\n"
     ]
    }
   ],
   "source": [
    "# RF testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = RandomForestRegressor(n_estimators = 50, \n",
    "                             random_state=0, n_jobs=-1)\n",
    "model_RF_pl = Pipeline([('standardize',StandardScaler()),('rf',algo)])\n",
    "model_RF_sk = model_RF_pl.fit(X, y)\n",
    "print('Sklearn prediction: ', model_RF_sk.predict(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_rf', model_RF_sk, X, y,'RF')\n",
    "test_model_rf = decrypt_json('test_model_rf')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_rf, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [ 1.40629809  0.51158577  0.89480592  1.71687482  1.47371355 -0.35973888\n",
      "  0.80332048  0.21089309  0.21089309  0.51158577]\n",
      "Custom prediction:  [1.4062980936482994, 0.5115857661944219, 0.8948059184685174, 1.716874822000581, 1.4737135512755717, -0.3597388764218241, 0.80332048251055, 0.21089309170890366, 0.21089309170890366, 0.5115857661944219]\n"
     ]
    }
   ],
   "source": [
    "# XGB testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = GradientBoostingRegressor(n_estimators = 10, \n",
    "                                 learning_rate = 0.1, random_state=0)\n",
    "model_GB_pl = Pipeline([('standardize',StandardScaler()),('gb',algo)])\n",
    "model_GB_sk = model_GB_pl.fit(X, y)\n",
    "print('Sklearn prediction: ', model_GB_sk.predict(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_xgb', model_GB_sk, X, y,'GradientBoost')\n",
    "test_model_xgb = decrypt_json('test_model_xgb')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_xgb, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [ 1.76405235  0.40537786  0.97873798  1.76405235  1.83305611 -0.97727788\n",
      "  0.95008842 -0.12728803 -0.12728803  0.4105985 ]\n",
      "Custom prediction:  [1.764052345967664, 0.4053778551527978, 0.9787379841057392, 1.764052345967664, 1.8330561087558663, -0.977277879876411, 0.9500884175255894, -0.12728803004562786, -0.12728803004562786, 0.41059850193837233]\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = AdaBoostRegressor(n_estimators = 10, \n",
    "                         learning_rate = 0.1, random_state=0)\n",
    "model_AGB_pl = Pipeline([('standardize',StandardScaler()),('agb',algo)])\n",
    "model_AGB_sk = model_AGB_pl.fit(X, y)\n",
    "print('Sklearn prediction: ', model_AGB_sk.predict(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_agb', model_AGB_sk, X, y,'AdaBoost')\n",
    "test_model_xgb = decrypt_json('test_model_agb')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_xgb, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
